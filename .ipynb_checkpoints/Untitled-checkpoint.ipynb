{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als eerste maken wij twee lookup tables, één met daarin de users en een lijst van alle films die zij hebben beoordeeld en één met alle films en een lisjt van alle users die die film hebben beoordeeld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11973, 14, 4.0, 943353278),\n",
       " (11973, 110, 5.0, 943354384),\n",
       " (11973, 161, 4.0, 943354197),\n",
       " (11973, 296, 5.0, 943354384),\n",
       " (11973, 300, 4.0, 943354345)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train = sc.textFile(\"data/ratings_train.csv\")\n",
    "first = ratings_train.first()\n",
    "ratings_train = ratings_train.filter(lambda x: x != first).map(lambda x : x.split(\"::\")).map(lambda x: (int(x[0]), int(x[1]), float(x[2]), int(x[3]))).filter(lambda x: x[2] > 3)\n",
    "ratings_train.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_ratings = ratings_train.map(lambda x: (x[1], x[2])).groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_movie_avg_rating_amount_reviewed = movie_ratings.map(lambda x: (x[0], (sum(list(x[1]))/len(list(x[1])), len(list(x[1]))))).filter(lambda x: x[1][1] > 4)\n",
    "good_movie_avg_amount_reviewed_lookup = good_movie_avg_rating_amount_reviewed.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4097, 178.5),\n",
       " (8194, 205.0),\n",
       " (25942, 25.5),\n",
       " (2057, 1487.5),\n",
       " (6154, 161.0),\n",
       " (5865, 27.5),\n",
       " (65552, 22.0),\n",
       " (17, 66583.5),\n",
       " (4114, 98.5),\n",
       " (77843, 406.5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_movies = good_movie_avg_rating_amount_reviewed.map(lambda x: (x[0], x[1][0] * x[1][1]))\n",
    "top_movies.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_movies_lookup = ratings_train.map(lambda x: (x[0], x[1])).groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings_test = sc.textFile(\"data/ratings_test.csv\")\n",
    "first = ratings_test.first()\n",
    "test_users = ratings_test.filter(lambda x: x != first).map(lambda x : x.split(\"::\")).map(lambda x: int(x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_seen_movies = user_movies_lookup.filter(lambda x: x[0] in test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49964"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_seen_movies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_seen_movies_lookup = user_seen_movies.map(lambda x: (x[0], list(x[1]))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top50 = top_movies.takeOrdered(50, key=lambda x: -x[1])\n",
    "recoms = sc.parallelize(test_users).map(lambda x: (x[0], top50))\n",
    "recoms_flat = recoms.flatMapValues(lambda x: x)\n",
    "user_recoms = recoms_flat.filter(lambda x: not(x[1][0] in user_seen_movies_lookup[x[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[31] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "user_recoms_amounts = user_recoms.groupByKey()\n",
    "user_recoms_amounts = user_recoms_amounts.map(lambda x: (x[0], len(list(x[1])))).filter(lambda x: x[1] != 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 116, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-10-4c83f657acfd>\", line 2, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-10-4c83f657acfd>\", line 2, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3a2ad47b231a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_recoms_lookup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_recoms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \"\"\"\n\u001b[0;32m-> 1536\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 116, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-10-4c83f657acfd>\", line 2, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\Users\\Joris\\Documents\\HBO-ICT\\Jaar2\\Periode7\\Spark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-10-4c83f657acfd>\", line 2, in <lambda>\nTypeError: 'int' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "user_recoms_lookup = user_recoms.groupByKey().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first = ratings_test.first()\n",
    "test_users_movies = ratings_test.filter(lambda x: x != first).map(lambda x : x.split(\"::\")).map(lambda x: (int(x[0]), int(x[1])))\n",
    "test_users_correct = test_users_movies.filter(lambda x: x[1] in user_recoms_lookup[x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recal = test_users_correct.count()\n",
    "total = test_users_movie.count()\n",
    "\n",
    "print(str(recal/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
